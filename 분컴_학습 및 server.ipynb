{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d1c2d5-b368-416e-b6eb-e04eefb5edba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Whispher 학습 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324aee64-8cb9-4811-bef9-522d4b83bdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from datasets import Dataset, Audio\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, WhisperConfig\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "# Custom callback to log training losses\n",
    "class LossLoggerCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.training_losses = []\n",
    "        self.eval_losses = []\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs:\n",
    "            if \"loss\" in logs:\n",
    "                self.training_losses.append(logs[\"loss\"])\n",
    "            if \"eval_loss\" in logs:\n",
    "                self.eval_losses.append(logs[\"eval_loss\"])\n",
    "\n",
    "# JSON 파일 로드\n",
    "data_path = ''\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 데이터셋 준비\n",
    "audio_paths, labels = [], []\n",
    "label_set = set()\n",
    "audio_base_path = \"\"\n",
    "all_audio_files = os.listdir(audio_base_path)\n",
    "\n",
    "for item in data:\n",
    "    file_upload_name = item['file_upload']\n",
    "    target_name = '-'.join(file_upload_name.split('-')[1:])\n",
    "    closest_match = [file for file in all_audio_files if file.endswith(target_name)]\n",
    "    if closest_match:\n",
    "        audio_paths.append(os.path.join(audio_base_path, closest_match[0]))\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No matching audio file found for {file_upload_name}\")\n",
    "    label = item['annotations'][0]['result'][0]['value']['choices'][0]\n",
    "    labels.append(label)\n",
    "    label_set.add(label)\n",
    "\n",
    "# Dataset 객체 생성\n",
    "dataset = Dataset.from_dict({'audio': audio_paths, 'text': labels})\n",
    "dataset = dataset.cast_column('audio', Audio(sampling_rate=16000))\n",
    "\n",
    "# Whisper 모델 및 프로세서 로드\n",
    "model_name = \"openai/whisper-small\"\n",
    "config = WhisperConfig.from_pretrained(model_name)\n",
    "config.dropout = 0.3\n",
    "config.attention_dropout = 0.3\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(model_name, language=\"ko\", task=\"transcribe\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name, config=config)\n",
    "model.to('cuda')\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    if not isinstance(audio[\"array\"], np.ndarray) or \"sampling_rate\" not in audio:\n",
    "        raise ValueError(\"오디오 데이터 형식이 올바르지 않습니다.\")\n",
    "    \n",
    "    batch[\"input_features\"] = processor(\n",
    "        audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]\n",
    "    ).input_features[0]\n",
    "    batch[\"labels\"] = processor(\n",
    "        text=batch[\"text\"], return_tensors=\"pt\", padding=True, truncation=True\n",
    "    ).input_ids.squeeze(0)\n",
    "    \n",
    "    return batch\n",
    "\n",
    "dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names)\n",
    "\n",
    "def verify_labels(batch):\n",
    "    if len(batch[\"labels\"]) == 0:\n",
    "        raise ValueError(\"빈 레이블이 발견되었습니다.\")\n",
    "    return batch\n",
    "\n",
    "dataset = dataset.map(verify_labels)\n",
    "\n",
    "split_dataset = dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "# 학습 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./whisper-child\",\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=500,\n",
    "    max_steps=3000,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    logging_steps=10,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=False,\n",
    "    push_to_hub=False,\n",
    "    optim=\"adamw_hf\"  # AdamW 옵티마이저 설정\n",
    ")\n",
    "\n",
    "loss_logger = LossLoggerCallback()\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    callbacks=[loss_logger]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained('')\n",
    "processor.save_pretrained('')\n",
    "\n",
    "# 손실 시각화\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(loss_logger.training_losses, label=\"Training Loss\")\n",
    "plt.plot(loss_logger.eval_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss & Validation Loss Over Time\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(\"training_loss_plot.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537b6b43-d396-406b-9ab5-aad057b31229",
   "metadata": {},
   "outputs": [],
   "source": [
    "#whispher mapping test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2576a6-92b0-4b59-b947-5170d5dd2911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import logging\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from jiwer import wer\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "model_path = './Telos_LLM_4'\n",
    "processor_path = './Telos_LLM_Processor_4'\n",
    "processor = WhisperProcessor.from_pretrained(processor_path)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_path)\n",
    "model.to('cuda')\n",
    "\n",
    "\n",
    "eval_audio_dir = './mapping_test/'\n",
    "eval_audio_paths = [os.path.join(eval_audio_dir, f) for f in os.listdir(eval_audio_dir) if f.endswith('.wav')]\n",
    "\n",
    "\n",
    "dog_breeds = {\"비숑\", \"말티즈\", \"닥스훈트\", \"불독\", \"푸들\", \"시츄\", \"웰시코기\", \"치와와\", \"보더콜리\", \"리트리버\"}\n",
    "cat_breeds = {\"스핑크스\", \"랙돌\", \"러시안블루\", \"먼치킨\", \"뱅갈\", \"샴\", \"아비시니안\", \"페르시안\", \"스코티시폴드\", \"터키시앙고라\"}\n",
    "\n",
    "true_transcriptions = [\n",
    "    (\"고양이\", \"cat\"), \n",
    "    (\"강아지\", \"dog\"),\n",
    "\n",
    "]\n",
    "\n",
    "def transcribe_audio(audio_path):\n",
    "    \"\"\"\n",
    "    Transcribe audio using the Whisper model.\n",
    "    \"\"\"\n",
    "    audio_input, _ = torchaudio.load(audio_path)\n",
    "    audio_input = audio_input.to('cuda')\n",
    "    \n",
    "\n",
    "    input_features = processor(audio_input.cpu().numpy(), sampling_rate=16000).input_features\n",
    "    input_features = torch.tensor(input_features).to('cuda')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predicted_ids = model.generate(input_features)\n",
    "    \n",
    "\n",
    "    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "    return transcription\n",
    "\n",
    "def classify_breed(transcription):\n",
    "\n",
    "    normalized_transcription = transcription.strip().lower()\n",
    "    \n",
    "\n",
    "    if any(breed.lower() in normalized_transcription for breed in dog_breeds):\n",
    "        return \"dog\"\n",
    "    elif any(breed.lower() in normalized_transcription for breed in cat_breeds):\n",
    "        return \"cat\"\n",
    "    return \"unknown\"\n",
    "\n",
    "\n",
    "correct = 0\n",
    "total = len(eval_audio_paths)\n",
    "\n",
    "for i, path in enumerate(eval_audio_paths):\n",
    "    transcription = transcribe_audio(path)\n",
    "    label = classify_breed(transcription)\n",
    "    \n",
    "\n",
    "    if i < len(true_transcriptions):\n",
    "        true_text, true_label = true_transcriptions[i]\n",
    "    else:\n",
    "        true_text, true_label = \"\", \"unknown\"  \n",
    "    \n",
    "\n",
    "    logging.info(f\"Audio file: {path}\")\n",
    "    logging.info(f\"Transcription: {transcription}\")\n",
    "    logging.info(f\"Normalized transcription: {transcription.strip().lower()}\")\n",
    "    logging.info(f\"Predicted label: {label}\")\n",
    "    logging.info(f\"Ground truth label: {true_label}\")\n",
    "    \n",
    "\n",
    "    if label == true_label:\n",
    "        correct += 1\n",
    "\n",
    "\n",
    "accuracy = correct / total\n",
    "logging.info(f\"Classification Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "\n",
    "all_true_text = \" \".join([t[0] for t in true_transcriptions])\n",
    "all_predicted_text = \" \".join([transcribe_audio(p) for p in eval_audio_paths])\n",
    "overall_wer = wer(all_true_text, all_predicted_text)\n",
    "logging.info(f\"Overall WER: {overall_wer:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec3c3a7-8903-402c-8208-5eebda992f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#llama3 학습 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8ffb6b-66e4-4738-8e42-753268f2fca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import logging\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# 로깅\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# GPU 쿠다 설정\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = '1'\n",
    "\n",
    "# hugging face api 토큰 설정\n",
    "hf_token = \"\"\n",
    "\n",
    "# 양자화 설정\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_threshold=6.0,\n",
    "    llm_int8_has_fp16_weight=False,\n",
    "    llm_int8_enable_fp32_cpu_offload=True,\n",
    ")\n",
    "\n",
    "# 토큰화된 모델 가져오기\n",
    "model_name = \"meta-llama/Llama-3.1-8b-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)\n",
    "\n",
    "# pad 토큰 추가\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# cpu or gpu 설정 코드\n",
    "device_map = {\"\": torch.cuda.current_device()}\n",
    "logger.info(f\"Using device map: {device_map}\")\n",
    "\n",
    "try:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,  \n",
    "        quantization_config=quantization_config,\n",
    "        token=hf_token,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "except torch.cuda.OutOfMemoryError as e:\n",
    "    logger.error(f\"OutOfMemoryError encountered: {e}\")\n",
    "    logger.info(\"Attempting to load model with CPU offload to save memory.\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"cpu\",  \n",
    "        quantization_config=quantization_config,\n",
    "        token=hf_token,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "# 토큰화된 모델에 맞춰서 임베딩화 진행\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "#LoRA 설정\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# 데이터셋 가져오기\n",
    "dataset = load_dataset(\"json\", data_files=\"script.json\", split=\"train\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "    )\n",
    "    tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"].copy()\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# 학습 설정 코드\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=2,  \n",
    "    gradient_accumulation_steps=32,  \n",
    "    num_train_epochs=30,\n",
    "    learning_rate=5e-5,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    save_steps=2000,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,  \n",
    "    bf16=False, \n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "try:\n",
    "    logger.info(\"Starting training...\")\n",
    "    trainer.train()\n",
    "    logger.info(\"Training completed.\")\n",
    "except RuntimeError as e:\n",
    "    logger.error(f\"RuntimeError encountered: {e}\")\n",
    "    torch.cuda.empty_cache()\n",
    "    raise e\n",
    "\n",
    "\n",
    "model.save_pretrained(\"./fine_tuned_llama7\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_llama7\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54273fe-2371-4ad7-9630-9ff2f6e8598e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#학습한 llama3 모델 로딩 후 챗봇 프롬프트 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85e34b8-f6b2-46fb-aaef-1eba4138e8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import logging\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import safetensors.torch\n",
    "\n",
    "# 로깅\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = '1'\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model_path = \"\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.1-8b-Instruct\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\", \n",
    ")\n",
    "\n",
    "\n",
    "if len(tokenizer) > model.config.vocab_size:\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "weights_path = f\"{model_path}/adapter_model.safetensors\"\n",
    "state_dict = safetensors.torch.load_file(weights_path)\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "\n",
    "chatbot = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "def chat_with_bot():\n",
    "    print(\"Chatbot is ready! Type 'exit' to end the conversation.\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Ending conversation. Goodbye!\")\n",
    "            break\n",
    "        response = chatbot(user_input, max_length=150, num_return_sequences=1, truncation=True, batch_size=1)\n",
    "        print(f\"Bot: {response[0]['generated_text']}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chat_with_bot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c19585-5290-4393-adee-50329f753285",
   "metadata": {},
   "outputs": [],
   "source": [
    "#whisper & llama3 통신 서버 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d2bfe0-1a71-416c-9292-d0f9a0190f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS \n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import bitsandbytes as bnb\n",
    "from waitress import serve  # Waitress를 사용한 타임아웃 설정\n",
    "\n",
    "# Flask 앱 설정\n",
    "app = Flask(__name__)\n",
    "CORS(app)  # CORS 활성화\n",
    "@app.after_request\n",
    "def add_header(response):\n",
    "    response.headers['Connection'] = 'keep-alive'\n",
    "    return response\n",
    "# 로깅 설정\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# GPU 사용 가능 여부 확인\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "# 모델 로드 및 GPU로 이동\n",
    "model_path = \"./fine_tuned_llama7\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.1-8b-Instruct\",\n",
    "    device_map=\"auto\",  # 모델을 GPU에 자동 분산\n",
    "    load_in_8bit=True   # 8-bit 양자화 사용\n",
    ")\n",
    "\n",
    "# 텍스트 생성 파이프라인\n",
    "chatbot = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)  # device 인수 제거\n",
    "\n",
    "@app.route('/chat', methods=['POST'])\n",
    "def chat():\n",
    "    # 요청 데이터 로깅\n",
    "    user_input = request.json.get('message')\n",
    "    logger.info(f\"Received request: {request.json}\")\n",
    "    \n",
    "    if user_input:\n",
    "        try:\n",
    "            # 텍스트 생성\n",
    "            response = chatbot(user_input, max_length=100, num_return_sequences=1)\n",
    "            generated_text = response[0]['generated_text']\n",
    "            \n",
    "            # 응답 데이터 로깅\n",
    "            logger.info(f\"Sending response: {generated_text}\")\n",
    "            return jsonify({\"response\": generated_text})\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during chatbot response generation: {e}\")\n",
    "            return jsonify({\"error\": \"Internal server error\"}), 500\n",
    "    \n",
    "    logger.warning(\"No message received in the request.\")\n",
    "    return jsonify({\"error\": \"No message received\"}), 400\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a399e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#naver TTS (public으로 공개하기 애매한 부분들이 많아 수정된 버전)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f203f909",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import urllib\n",
    "import urllib2\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "client_id = \"클라이언트 id\"\n",
    "text = unicode(\"#llama script 인자\")\n",
    "speaker = \"스피커\"\n",
    "speed = \"0\"\n",
    "volume = \"0\"\n",
    "pitch = \"0\"\n",
    "fmt = \"mp3\"\n",
    "val = {\n",
    "    \"speaker\": speaker,\n",
    "    \"volume\": volume,\n",
    "    \"speed\":speed,\n",
    "    \"pitch\": pitch,\n",
    "    \"text\":text,\n",
    "    \"format\": fmt\n",
    "}\n",
    "data = urllib.urlencode(val)\n",
    "url = \"tts 관리 주소\"\n",
    "headers = {\n",
    "    \"X-NCP-APIGW-API-KEY-ID\" : client_id,\n",
    "    \"X-NCP-APIGW-API-KEY\" : client_secret\n",
    "}\n",
    "request = urllib2.Request(url, data, headers)\n",
    "response = urllib2.urlopen(request)\n",
    "rescode = response.getcode()\n",
    "if(rescode==200):\n",
    "    print(\"TTS mp3 save\")\n",
    "    response_body = response.read()\n",
    "    with open('.mp3', 'wb') as f:\n",
    "        f.write(response_body)\n",
    "else:\n",
    "    print(\"Error Code:\" + rescode)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "llama3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
